{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline (Module 1 -> 2 -> 3) on Colab\n",
    "\n",
    "This notebook runs the complete system:\n",
    "1. Module 1 Extraction\n",
    "2. Module 2 Perception (ASR + captions)\n",
    "3. Module 3 Reasoning NLP (G1->G8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
     "from google.colab import drive\n",
     "\n",
     "drive.mount('/content/drive')\n",
     "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_CACHE_ROOT = Path('/content/drive/MyDrive/video-summary/model_cache')\n",
    "HF_HOME_DIR = DRIVE_CACHE_ROOT / 'huggingface'\n",
    "XDG_CACHE_DIR = DRIVE_CACHE_ROOT / 'xdg'\n",
    "TORCH_HOME_DIR = DRIVE_CACHE_ROOT / 'torch'\n",
    "\n",
    "for path in [HF_HOME_DIR, XDG_CACHE_DIR, TORCH_HOME_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ['HF_HOME'] = str(HF_HOME_DIR)\n",
    "os.environ['HF_HUB_CACHE'] = str(HF_HOME_DIR / 'hub')\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(HF_HOME_DIR / 'transformers')\n",
    "os.environ['XDG_CACHE_HOME'] = str(XDG_CACHE_DIR)\n",
    "os.environ['TORCH_HOME'] = str(TORCH_HOME_DIR)\n",
    "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
    "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
    "os.environ.setdefault('VIDEO_SUMMARY_LOCAL_4BIT', '1')\n",
    "\n",
    "Path(os.environ['HF_HUB_CACHE']).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.environ['TRANSFORMERS_CACHE']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Model cache root:', DRIVE_CACHE_ROOT)\n",
    "print('HF_HOME =', os.environ['HF_HOME'])\n",
    "print('HF_HUB_CACHE =', os.environ['HF_HUB_CACHE'])\n",
    "print('TRANSFORMERS_CACHE =', os.environ['TRANSFORMERS_CACHE'])\n",
    "print('XDG_CACHE_HOME =', os.environ['XDG_CACHE_HOME'])\n",
    "print('TORCH_HOME =', os.environ['TORCH_HOME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def _build_backend_chain(primary: str, fallback: str, last_resort: str) -> list[str]:\n",
    "    chain = []\n",
    "    for item in [primary, fallback, last_resort]:\n",
    "        if item and item not in chain:\n",
    "            chain.append(item)\n",
    "    return chain\n",
    "\n",
    "def _select_chain_by_preflight(chain: list[str], preflight: dict) -> list[str]:\n",
    "    ready = []\n",
    "    for b in chain:\n",
    "        if b == 'local':\n",
    "            if preflight.get('local_ready', False):\n",
    "                ready.append(b)\n",
    "        elif b == 'api':\n",
    "            if preflight.get('api_ready', False):\n",
    "                ready.append(b)\n",
    "        else:\n",
    "            ready.append(b)\n",
    "    return ready\n",
    "\n",
    "def _run_pipeline_with_backend_chain(base_cmd: list[str], backend_chain: list[str]) -> None:\n",
    "    def _diagnose_failure(text: str) -> str:\n",
    "        lowered = (text or '').lower()\n",
    "        if 'no space left on device' in lowered or 'insufficient free drive space' in lowered:\n",
    "            return 'storage_full'\n",
    "        if 'cuda out of memory' in lowered or 'cublas' in lowered:\n",
    "            return 'gpu_oom'\n",
    "        if 'connection' in lowered or 'timeout' in lowered or 'read timed out' in lowered:\n",
    "            return 'download_or_network'\n",
    "        if 'local backend requested but preflight failed' in lowered:\n",
    "            return 'preflight_blocked'\n",
    "        if 'response is not a valid json object' in lowered:\n",
    "            return 'json_parse_failure'\n",
    "        if '401' in lowered or '403' in lowered:\n",
    "            return 'auth_or_access'\n",
    "        return 'unknown'\n",
    "\n",
    "    collected = []\n",
    "    max_tail_lines = 400\n",
    "    for idx, backend in enumerate(backend_chain):\n",
    "        next_backend = backend_chain[idx + 1] if idx + 1 < len(backend_chain) else ''\n",
    "        cmd_try = list(base_cmd) + ['--summarize-backend', backend]\n",
    "        if next_backend:\n",
    "            cmd_try += ['--summarize-fallback-backend', next_backend]\n",
    "        fallback_label = next_backend if next_backend else 'none'\n",
    "        print(f'Attempt {idx+1}/{len(backend_chain)} with backend={backend}, fallback={fallback_label}')\n",
    "\n",
    "        proc = subprocess.Popen(\n",
    "            cmd_try,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "        )\n",
    "\n",
    "        tail_lines = []\n",
    "        if proc.stdout is not None:\n",
    "            for line in proc.stdout:\n",
    "                print(line, end='')\n",
    "                tail_lines.append(line)\n",
    "                if len(tail_lines) > max_tail_lines:\n",
    "                    tail_lines = tail_lines[-max_tail_lines:]\n",
    "\n",
    "        return_code = proc.wait()\n",
    "        tail_text = ''.join(tail_lines)\n",
    "\n",
    "        if return_code == 0:\n",
    "            print(f'Pipeline success with backend={backend}')\n",
    "            return\n",
    "\n",
    "        collected.append((backend, return_code, tail_text))\n",
    "        print('Diagnosed failure category =', _diagnose_failure(tail_text))\n",
    "        print(f'Pipeline failed with backend={backend}, returncode={return_code}')\n",
    "        if tail_text:\n",
    "            print('--- combined output tail ---')\n",
    "            print(tail_text[-5000:])\n",
    "\n",
    "    lines = ['All backend attempts failed:']\n",
    "    for backend, code, tail_text in collected:\n",
    "        last_line = tail_text.strip().splitlines()[-1] if tail_text.strip() else f'returncode={code}'\n",
    "        lines.append(f'- {backend}: {last_line} [category={_diagnose_failure(tail_text)}]')\n",
    "    raise RuntimeError('\\n'.join(lines))\n",
    "from pathlib import Path\n",
    "\n",
    "def _ensure_ffmpeg():\n",
    "    if Path('/usr/bin/ffmpeg').exists():\n",
    "        print('ffmpeg already installed')\n",
    "        return\n",
    "    subprocess.check_call(['apt-get', 'update', '-y'])\n",
    "    subprocess.check_call(['apt-get', 'install', '-y', 'ffmpeg'])\n",
    "\n",
    "def _ensure_packages():\n",
    "    req = {\n",
    "        'scenedetect': 'scenedetect',\n",
    "        'cv2': 'opencv-python-headless',\n",
    "        'faster_whisper': 'faster-whisper',\n",
    "        'transformers': 'transformers>=4.45.0',\n",
    "        'accelerate': 'accelerate>=0.34.0',\n",
    "        'sentencepiece': 'sentencepiece>=0.2.0',\n",
    "        'bitsandbytes': 'bitsandbytes>=0.43.0',\n",
    "        'PIL': 'pillow',\n",
    "        'tqdm': 'tqdm',\n",
    "        'jsonschema': 'jsonschema',\n",
    "    }\n",
    "    miss = [pip for mod,pip in req.items() if importlib.util.find_spec(mod) is None]\n",
    "    if miss:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', *miss])\n",
    "\n",
    "    critical = [\n",
    "        'transformers>=4.45.0',\n",
    "        'accelerate>=0.34.0',\n",
    "        'sentencepiece>=0.2.0',\n",
    "        'safetensors>=0.4.3',\n",
    "        'huggingface_hub>=0.24.0',\n",
    "        'bitsandbytes>=0.43.0',\n",
    "    ]\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '--upgrade', *critical])\n",
    "\n",
    "def _print_runtime_versions():\n",
    "    pkg_names = ['torch', 'transformers', 'accelerate', 'sentencepiece', 'huggingface_hub', 'safetensors']\n",
    "    for name in pkg_names:\n",
    "        try:\n",
    "            ver = importlib.metadata.version(name)\n",
    "            print(f'{name}={ver}')\n",
    "        except Exception:\n",
    "            print(f'{name}=<missing>')\n",
    "\n",
    "_ensure_ffmpeg()\n",
    "_ensure_packages()\n",
    "_print_runtime_versions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_DIR = Path('/content/video-summary')\n",
    "BRANCH_NAME = os.environ.get('VIDEO_SUMMARY_BRANCH', '02-member-2-reasoning-nlp')\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    subprocess.check_call([\n",
    "        'git', 'clone', '--single-branch', '--branch', BRANCH_NAME,\n",
    "        'https://github.com/TCTri205/video-summary.git', str(REPO_DIR)\n",
    "    ])\n",
    "else:\n",
    "    os.chdir(REPO_DIR)\n",
    "    subprocess.check_call(['git', 'fetch', 'origin'])\n",
    "    subprocess.check_call(['git', 'checkout', BRANCH_NAME])\n",
    "    subprocess.check_call(['git', 'pull', 'origin', BRANCH_NAME])\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/video-summary')\n",
    "INPUT_VIDEO_DRIVE = DRIVE_ROOT / 'input' / 'raw_video.mp4'\n",
    "PROCESSED_DRIVE = DRIVE_ROOT / 'processed'\n",
    "\n",
    "LOCAL_ROOT = Path('/content/video-summary-work')\n",
    "LOCAL_INPUT_DIR = LOCAL_ROOT / 'input'\n",
    "LOCAL_PROCESSED = LOCAL_ROOT / 'processed'\n",
    "LOCAL_INPUT_VIDEO = LOCAL_INPUT_DIR / INPUT_VIDEO_DRIVE.name\n",
    "\n",
    "for path in [DRIVE_ROOT, PROCESSED_DRIVE, LOCAL_INPUT_DIR, LOCAL_PROCESSED]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input selection policy (mac dinh: fixed de de debug va so sanh).\n",
    "SOURCE_VIDEO_DIR = Path('/content/drive/MyDrive/Final-Project-AI-Sgroup/Video')\n",
    "INPUT_SELECTION_MODE = os.environ.get('VIDEO_SUMMARY_INPUT_MODE', 'fixed').strip().lower()  # fixed|first|random\n",
    "FIXED_SOURCE_VIDEO = Path(os.environ.get('VIDEO_SUMMARY_FIXED_SOURCE', str(INPUT_VIDEO_DRIVE)))\n",
    "\n",
    "def _video_files(folder: Path):\n",
    "    exts = {'.mp4', '.mov', '.mkv', '.avi', '.webm', '.m4v'}\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    return [p for p in folder.rglob('*') if p.is_file() and p.suffix.lower() in exts]\n",
    "\n",
    "selected = None\n",
    "if INPUT_SELECTION_MODE == 'fixed':\n",
    "    selected = FIXED_SOURCE_VIDEO\n",
    "    if not selected.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f'Khong tim thay fixed source video: {selected}. '\n",
    "            'Dat VIDEO_SUMMARY_FIXED_SOURCE hoac doi INPUT_SELECTION_MODE sang first/random.'\n",
    "        )\n",
    "elif INPUT_SELECTION_MODE in {'first', 'random'}:\n",
    "    candidates = sorted(_video_files(SOURCE_VIDEO_DIR))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f'Khong tim thay video trong thu muc: {SOURCE_VIDEO_DIR}. '\n",
    "            'Hay kiem tra lai duong dan va upload it nhat 1 file video hop le.'\n",
    "        )\n",
    "    selected = candidates[0] if INPUT_SELECTION_MODE == 'first' else random.choice(candidates)\n",
    "else:\n",
    "    raise ValueError(f'INPUT_SELECTION_MODE khong hop le: {INPUT_SELECTION_MODE}')\n",
    "INPUT_VIDEO_DRIVE.parent.mkdir(parents=True, exist_ok=True)\n",
    "if selected.resolve() != INPUT_VIDEO_DRIVE.resolve():\n",
    "    shutil.copy2(selected, INPUT_VIDEO_DRIVE)\n",
    "print(f'Input selection mode: {INPUT_SELECTION_MODE}')\n",
    "print(f'Selected source video: {selected}')\n",
    "print(f'Copied to pipeline input: {INPUT_VIDEO_DRIVE}')\n",
    "\n",
    "if (not LOCAL_INPUT_VIDEO.exists()) or (LOCAL_INPUT_VIDEO.stat().st_size != INPUT_VIDEO_DRIVE.stat().st_size):\n",
    "    shutil.copy2(INPUT_VIDEO_DRIVE, LOCAL_INPUT_VIDEO)\n",
    "\n",
    "VIDEO_PATH = str(LOCAL_INPUT_VIDEO)\n",
    "OUTPUT_ROOT = str(LOCAL_PROCESSED)\n",
    "VIDEO_NAME = Path(VIDEO_PATH).stem\n",
    "print('VIDEO_NAME =', VIDEO_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import importlib.metadata\n",
    "import importlib.util\n",
    "\n",
    "# Runtime config\n",
    "REPLAY_MODE = False\n",
    "CAPTION_BATCH_SIZE = 4\n",
    "SUMMARIZE_MAX_NEW_TOKENS = 384\n",
    "SUMMARIZE_PROMPT_MAX_CHARS = int(os.environ.get('VIDEO_SUMMARY_SUMMARIZE_PROMPT_MAX_CHARS', '9000'))\n",
    "RUN_LOCAL_SMOKE_TEST = os.environ.get('VIDEO_SUMMARY_RUN_LOCAL_SMOKE_TEST', '0').strip() == '1'\n",
    "SUMMARIZE_BACKEND = os.environ.get('VIDEO_SUMMARY_SUMMARIZE_BACKEND', 'local').strip().lower()\n",
    "SUMMARIZE_FALLBACK_BACKEND = os.environ.get('VIDEO_SUMMARY_SUMMARIZE_FALLBACK_BACKEND', 'api').strip().lower()\n",
    "SUMMARIZE_LAST_RESORT_BACKEND = os.environ.get('VIDEO_SUMMARY_SUMMARIZE_LAST_RESORT_BACKEND', 'api').strip().lower()\n",
    "LOCAL_MODEL_VERSION = os.environ.get('VIDEO_SUMMARY_LOCAL_MODEL_VERSION', 'Qwen/Qwen2.5-3B-Instruct').strip()\n",
    "QC_ENFORCE_THRESHOLDS = os.environ.get('VIDEO_SUMMARY_QC_ENFORCE_THRESHOLDS', '1').strip() == '1'\n",
    "_valid_backends = {'api', 'local'}\n",
    "if (\n",
    "    SUMMARIZE_BACKEND not in _valid_backends\n",
    "    or SUMMARIZE_FALLBACK_BACKEND not in _valid_backends\n",
    "    or SUMMARIZE_LAST_RESORT_BACKEND not in _valid_backends\n",
    "):\n",
    "    raise ValueError('Invalid summarize backend. Supported: api, local')\n",
    "if not LOCAL_MODEL_VERSION:\n",
    "    raise ValueError('VIDEO_SUMMARY_LOCAL_MODEL_VERSION must not be empty')\n",
    "\n",
    "def _preflight_backend_support() -> dict:\n",
    "    has_transformers = importlib.util.find_spec('transformers') is not None\n",
    "    has_torch = importlib.util.find_spec('torch') is not None\n",
    "    has_accelerate = importlib.util.find_spec('accelerate') is not None\n",
    "    has_bitsandbytes = importlib.util.find_spec('bitsandbytes') is not None\n",
    "    has_sentencepiece = importlib.util.find_spec('sentencepiece') is not None\n",
    "    local_deps_ready = has_transformers and has_torch and has_accelerate and has_sentencepiece and has_bitsandbytes\n",
    "    has_api_base = bool(os.environ.get('OPENAI_BASE_URL', '').strip())\n",
    "    has_api_key = bool(os.environ.get('OPENAI_API_KEY', '').strip())\n",
    "    api_ready = has_api_base and has_api_key\n",
    "    local_ready = local_deps_ready\n",
    "    local_probe_error = ''\n",
    "    if local_ready:\n",
    "        try:\n",
    "            import torch\n",
    "            from transformers import AutoTokenizer\n",
    "            _ = AutoTokenizer.from_pretrained(LOCAL_MODEL_VERSION)\n",
    "            local_ready = torch.cuda.is_available()\n",
    "            if not local_ready:\n",
    "                local_probe_error = 'CUDA not available on current runtime'\n",
    "        except Exception as exc:\n",
    "            local_ready = False\n",
    "            local_probe_error = str(exc)\n",
    "    else:\n",
    "        local_probe_error = 'missing deps: torch/transformers/accelerate/sentencepiece/bitsandbytes'\n",
    "\n",
    "    if not os.environ.get('OPENAI_MODEL', '').strip():\n",
    "        os.environ['OPENAI_MODEL'] = LOCAL_MODEL_VERSION\n",
    "    return {\n",
    "        'local_ready': local_ready,\n",
    "        'api_ready': api_ready,\n",
    "        'has_api_base': has_api_base,\n",
    "        'has_api_key': has_api_key,\n",
    "        'local_probe_error': local_probe_error,\n",
    "    }\n",
    "\n",
    "BACKEND_PREFLIGHT = _preflight_backend_support()\n",
    "\n",
    "RAW_VIDEO_LOCAL = str(LOCAL_INPUT_VIDEO)\n",
    "RAW_VIDEO_DRIVE = str(INPUT_VIDEO_DRIVE)\n",
    "\n",
    "LOCAL_ARTIFACTS = LOCAL_PROCESSED / 'artifacts'\n",
    "ARTIFACTS_DRIVE = DRIVE_ROOT / 'artifacts'\n",
    "LOCAL_DELIVERABLES = LOCAL_PROCESSED / 'deliverables'\n",
    "DELIVERABLES_DRIVE = DRIVE_ROOT / 'deliverables'\n",
    "LOCAL_ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_DRIVE.mkdir(parents=True, exist_ok=True)\n",
    "LOCAL_DELIVERABLES.mkdir(parents=True, exist_ok=True)\n",
    "DELIVERABLES_DRIVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_ID = datetime.now().strftime('colab_full_%Y%m%d_%H%M%S')\n",
    "\n",
    "CLEAN_HEAVY_EXTRACTION = False\n",
    "CLEAN_OLD_RUNS = True\n",
    "KEEP_LAST_RUNS = 3\n",
    "\n",
    "print('REPLAY_MODE =', REPLAY_MODE)\n",
    "print('RUN_ID =', RUN_ID)\n",
    "print('SUMMARIZE_BACKEND =', SUMMARIZE_BACKEND)\n",
    "print('SUMMARIZE_FALLBACK_BACKEND =', SUMMARIZE_FALLBACK_BACKEND)\n",
    "print('SUMMARIZE_LAST_RESORT_BACKEND =', SUMMARIZE_LAST_RESORT_BACKEND)\n",
    "print('LOCAL_MODEL_VERSION =', LOCAL_MODEL_VERSION)\n",
    "print('SUMMARIZE_PROMPT_MAX_CHARS =', SUMMARIZE_PROMPT_MAX_CHARS)\n",
    "print('RUN_LOCAL_SMOKE_TEST =', RUN_LOCAL_SMOKE_TEST)\n",
    "print('VIDEO_SUMMARY_LOCAL_4BIT =', os.environ.get('VIDEO_SUMMARY_LOCAL_4BIT', '1'))\n",
    "print('BACKEND_PREFLIGHT =', BACKEND_PREFLIGHT)\n",
    "if SUMMARIZE_BACKEND == 'local' and not BACKEND_PREFLIGHT.get('local_ready', False):\n",
    "    raise RuntimeError(f\"Local backend requested but preflight failed: {BACKEND_PREFLIGHT.get('local_probe_error', 'unknown')}\")\n",
    "if SUMMARIZE_BACKEND == 'local':\n",
    "    import shutil\n",
    "    usage = shutil.disk_usage('/content/drive')\n",
    "    free_gb = usage.free / (1024**3)\n",
    "    print('Drive free space (GB)=', round(free_gb, 2))\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            free_vram, total_vram = torch.cuda.mem_get_info()\n",
    "            print('GPU VRAM free/total (GB)=', round(free_vram / (1024**3), 2), '/', round(total_vram / (1024**3), 2))\n",
    "    except Exception as _exc:\n",
    "        print('VRAM probe skipped:', _exc)\n",
    "    if free_gb < 15:\n",
    "        raise RuntimeError('Insufficient free Drive space for local Qwen cache (need >= 15GB).')\n",
    "print('QC_ENFORCE_THRESHOLDS =', QC_ENFORCE_THRESHOLDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quickdiag01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.metadata\n",
    "import shutil\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def _quick_diagnose_status() -> None:\n",
    "    usage = shutil.disk_usage('/content/drive')\n",
    "    free_gb = usage.free / (1024**3)\n",
    "    hf_cache = Path(os.environ.get('HF_HUB_CACHE', '/content/.cache/huggingface/hub'))\n",
    "    cache_size_gb = 0.0\n",
    "    try:\n",
    "        if hf_cache.exists():\n",
    "            cache_size_gb = sum(p.stat().st_size for p in hf_cache.rglob('*') if p.is_file()) / (1024**3)\n",
    "    except Exception:\n",
    "        cache_size_gb = -1.0\n",
    "\n",
    "    versions = {}\n",
    "    for pkg in ['torch', 'transformers', 'accelerate', 'sentencepiece', 'huggingface_hub', 'safetensors', 'bitsandbytes']:\n",
    "        try:\n",
    "            versions[pkg] = importlib.metadata.version(pkg)\n",
    "        except Exception:\n",
    "            versions[pkg] = '<missing>'\n",
    "\n",
    "    rows = [\n",
    "        ('backend', SUMMARIZE_BACKEND),\n",
    "        ('local_model', LOCAL_MODEL_VERSION),\n",
    "        ('local_ready', str(BACKEND_PREFLIGHT.get('local_ready'))),\n",
    "        ('local_probe_error', str(BACKEND_PREFLIGHT.get('local_probe_error', '')) or '-'),\n",
    "        ('api_ready', str(BACKEND_PREFLIGHT.get('api_ready'))),\n",
    "        ('drive_free_gb', f'{free_gb:.2f}'),\n",
    "        ('hf_cache_exists', str(hf_cache.exists())),\n",
    "        ('hf_cache_size_gb', f'{cache_size_gb:.2f}' if cache_size_gb >= 0 else 'unknown'),\n",
    "        ('torch_cuda', str(torch.cuda.is_available())),\n",
    "    ]\n",
    "\n",
    "    print('--- Quick Diagnose ---')\n",
    "    for k, v in rows:\n",
    "        print(f'{k:>18}: {v}')\n",
    "    print('--- Runtime Versions ---')\n",
    "    for pkg, ver in versions.items():\n",
    "        print(f'{pkg:>18}: {ver}')\n",
    "\n",
    "_quick_diagnose_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29902df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _run_local_qwen_smoke_test(model_name: str) -> None:\n",
    "    if SUMMARIZE_BACKEND != 'local':\n",
    "        print('Skip local smoke test because summarize backend is not local')\n",
    "        return\n",
    "\n",
    "    print('--- Local Qwen smoke test ---')\n",
    "    print('CUDA available =', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('GPU =', torch.cuda.get_device_name(0))\n",
    "\n",
    "    from reasoning_nlp.summarizer.llm_client import _local_transformers_completion\n",
    "\n",
    "    payload, latency_ms, token_count = _local_transformers_completion(\n",
    "        prompt='[00:00:01.000] Nhan vat mo cua buoc vao phong va noi chuyen ngan gon voi ban cua minh.',\n",
    "        model_name=model_name,\n",
    "        timeout_ms=30000,\n",
    "        max_new_tokens=160,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    print('Smoke test latency_ms =', latency_ms)\n",
    "    print('Smoke test token_count =', token_count)\n",
    "    print('Smoke test keys =', sorted(payload.keys()))\n",
    "\n",
    "if RUN_LOCAL_SMOKE_TEST:\n",
    "    _run_local_qwen_smoke_test(LOCAL_MODEL_VERSION)\n",
    "else:\n",
    "    print('Skip local smoke test by default to avoid duplicate GPU residency before subprocess run')\n",
    "\n",
    "from extraction_perception.extraction.extraction import VideoPreprocessor\n",
    "\n",
    "if REPLAY_MODE:\n",
    "    print('Replay mode enabled: skip Module 1 extraction')\n",
    "else:\n",
    "    processor = VideoPreprocessor(video_path=RAW_VIDEO_LOCAL, output_root=str(LOCAL_PROCESSED))\n",
    "    timestamps = processor.detect_scenes()\n",
    "    audio_path = processor.extract_audio()\n",
    "    metadata = processor.extract_keyframes_and_metadata(timestamps)\n",
    "    print('Scenes:', len(timestamps))\n",
    "    print('Audio:', audio_path)\n",
    "    print('Keyframes:', metadata.get('total_keyframes', 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb555e",
   "metadata": {},
   "source": [
    "## Step 1 - Module 1 Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f469f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from extraction_perception.extraction.whisper_module import WhisperExtractor\n",
    "from extraction_perception.perception.caption import VisualCaptioner\n",
    "\n",
    "LOCAL_EXTRACTION_DIR = LOCAL_PROCESSED / VIDEO_NAME / 'extraction'\n",
    "AUDIO_PATH = LOCAL_EXTRACTION_DIR / 'audio' / 'audio_16k.wav'\n",
    "METADATA_PATH = LOCAL_EXTRACTION_DIR / 'scene_metadata.json'\n",
    "CAPTIONS_PATH = LOCAL_EXTRACTION_DIR / 'visual_captions.json'\n",
    "\n",
    "if REPLAY_MODE:\n",
    "    print('Replay mode enabled: skip Module 2 perception')\n",
    "else:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    compute_type = 'float16' if device == 'cuda' else 'int8'\n",
    "\n",
    "    asr = WhisperExtractor(model_size='base', device=device, compute_type=compute_type)\n",
    "    asr.transcribe(\n",
    "        input_path=str(AUDIO_PATH),\n",
    "        language='vi',\n",
    "        output_root=str(LOCAL_PROCESSED),\n",
    "        output_name=VIDEO_NAME,\n",
    "    )\n",
    "\n",
    "    captioner = VisualCaptioner()\n",
    "    captioner.caption_from_metadata(\n",
    "        metadata_path=str(METADATA_PATH),\n",
    "        output_path=str(CAPTIONS_PATH),\n",
    "        batch_size=CAPTION_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    del asr\n",
    "    del captioner\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35077b30",
   "metadata": {},
   "source": [
    "## Step 2 - Module 2 Perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "DRIVE_EXTRACTION_DIR = PROCESSED_DRIVE / VIDEO_NAME / 'extraction'\n",
    "DRIVE_EXTRACTION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not REPLAY_MODE:\n",
    "    sync_to_drive = [\n",
    "        ('scene_metadata.json', 'scene_metadata.json'),\n",
    "        ('audio_transcripts.json', 'audio_transcripts.json'),\n",
    "        ('visual_captions.json', 'visual_captions.json'),\n",
    "    ]\n",
    "    for src_rel, dst_rel in sync_to_drive:\n",
    "        src = LOCAL_EXTRACTION_DIR / src_rel\n",
    "        dst = DRIVE_EXTRACTION_DIR / dst_rel\n",
    "        if src.exists():\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "AUDIO_TRANSCRIPTS = DRIVE_EXTRACTION_DIR / 'audio_transcripts.json'\n",
    "VISUAL_CAPTIONS = DRIVE_EXTRACTION_DIR / 'visual_captions.json'\n",
    "\n",
    "if not AUDIO_TRANSCRIPTS.exists() or not VISUAL_CAPTIONS.exists():\n",
    "    raise FileNotFoundError('Missing perception outputs on Drive for reasoning stage')\n",
    "\n",
    "print('Drive transcripts/captions ready')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e095ca",
   "metadata": {},
   "source": [
    "## Step 3 - Module 3 Reasoning (G1->G8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114866dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "artifacts_root = ARTIFACTS_DRIVE if REPLAY_MODE else LOCAL_ARTIFACTS\n",
    "deliverables_root = DELIVERABLES_DRIVE if REPLAY_MODE else LOCAL_DELIVERABLES\n",
    "print('artifacts_root =', artifacts_root)\n",
    "print('deliverables_root =', deliverables_root)\n",
    "backend_plan = _build_backend_chain(SUMMARIZE_BACKEND, SUMMARIZE_FALLBACK_BACKEND, SUMMARIZE_LAST_RESORT_BACKEND)\n",
    "selected_chain = _select_chain_by_preflight(backend_plan, BACKEND_PREFLIGHT)\n",
    "print('Backend priority plan =', backend_plan)\n",
    "print('Backend chain after preflight =', selected_chain)\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    '-m',\n",
    "    'reasoning_nlp.pipeline_runner',\n",
    "    '--audio-transcripts', str(AUDIO_TRANSCRIPTS),\n",
    "    '--visual-captions', str(VISUAL_CAPTIONS),\n",
    "    '--raw-video', RAW_VIDEO_DRIVE,\n",
    "    '--stage', 'g8',\n",
    "    '--run-id', RUN_ID,\n",
    "    '--artifacts-root', str(artifacts_root),\n",
    "    '--deliverables-root', str(deliverables_root),\n",
    "    '--model-version', LOCAL_MODEL_VERSION,\n",
    "    '--summarize-max-new-tokens', str(SUMMARIZE_MAX_NEW_TOKENS),\n",
    "    '--summarize-prompt-max-chars', str(SUMMARIZE_PROMPT_MAX_CHARS),\n",
    "]\n",
    "if QC_ENFORCE_THRESHOLDS:\n",
    "    cmd.append('--qc-enforce-thresholds')\n",
    "if REPLAY_MODE:\n",
    "    cmd.append('--replay')\n",
    "\n",
    "_run_pipeline_with_backend_chain(cmd, selected_chain)\n",
    "print('Reasoning pipeline completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "source_run_dir = (ARTIFACTS_DRIVE if REPLAY_MODE else LOCAL_ARTIFACTS) / RUN_ID\n",
    "drive_run_dir = ARTIFACTS_DRIVE / RUN_ID\n",
    "source_deliverable_dir = (DELIVERABLES_DRIVE if REPLAY_MODE else LOCAL_DELIVERABLES) / RUN_ID\n",
    "drive_deliverable_dir = DELIVERABLES_DRIVE / RUN_ID\n",
    "\n",
    "def _copy_item(src: Path, dst: Path) -> None:\n",
    "    if not src.exists():\n",
    "        return\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if src.is_dir():\n",
    "        if dst.exists():\n",
    "            shutil.rmtree(dst)\n",
    "        shutil.copytree(src, dst)\n",
    "    else:\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "keep_rel_paths = [\n",
    "    'run_meta.json',\n",
    "    'g1_validate/normalized_input.json',\n",
    "    'g2_align/alignment_result.json',\n",
    "    'g3_context/context_blocks.json',\n",
    "    'g4_summarize/parse_meta.json',\n",
    "    'g4_summarize/summary_script.internal.json',\n",
    "    'g5_segment/summary_script.json',\n",
    "    'g5_segment/summary_video_manifest.json',\n",
    "    'g6_manifest/manifest_validation.json',\n",
    "    'g7_assemble/render_meta.json',\n",
    "    'g7_assemble/summary_video.mp4',\n",
    "    'g8_qc/quality_report.json',\n",
    "]\n",
    "for rel in keep_rel_paths:\n",
    "    _copy_item(source_run_dir / rel, drive_run_dir / rel)\n",
    "for rel in ['summary_video.mp4', 'summary_text.txt']:\n",
    "    _copy_item(source_deliverable_dir / rel, drive_deliverable_dir / rel)\n",
    "\n",
    "if CLEAN_HEAVY_EXTRACTION and DRIVE_EXTRACTION_DIR.exists():\n",
    "    heavy_items = [\n",
    "        DRIVE_EXTRACTION_DIR / 'keyframes',\n",
    "        DRIVE_EXTRACTION_DIR / 'audio' / 'audio_16k.wav',\n",
    "    ]\n",
    "    for item in heavy_items:\n",
    "        if item.is_dir():\n",
    "            shutil.rmtree(item, ignore_errors=True)\n",
    "        elif item.exists():\n",
    "            item.unlink()\n",
    "\n",
    "if CLEAN_OLD_RUNS and KEEP_LAST_RUNS > 0:\n",
    "    all_runs = [p for p in ARTIFACTS_DRIVE.iterdir() if p.is_dir() and p.name.startswith('colab_full_')]\n",
    "    all_runs.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    for old in all_runs[KEEP_LAST_RUNS:]:\n",
    "        if old.name != RUN_ID:\n",
    "            shutil.rmtree(old, ignore_errors=True)\n",
    "\n",
    "print('Synced balanced artifacts to Drive:', drive_run_dir)\n",
    "print('Synced final deliverables to Drive:', drive_deliverable_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "RUN_DIR = ARTIFACTS_DRIVE / RUN_ID\n",
    "FINAL_DIR = DELIVERABLES_DRIVE / RUN_ID\n",
    "OUTPUT_TEXT = FINAL_DIR / 'summary_text.txt'\n",
    "INTERNAL_SUMMARY = RUN_DIR / 'g4_summarize' / 'summary_script.internal.json'\n",
    "ALIGNMENT = RUN_DIR / 'g2_align' / 'alignment_result.json'\n",
    "REPORT = RUN_DIR / 'g8_qc' / 'quality_report.json'\n",
    "\n",
    "def _safe_json(path):\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding='utf-8'))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _split_sentences(text):\n",
    "    chunks = [x.strip() for x in re.split(r'[\\n\\r]+|(?<=[.!?])\\s+', text) if x.strip()]\n",
    "    return chunks\n",
    "\n",
    "def _asciiish_ratio(text):\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text)\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    asciiish = sum(1 for w in words if all(ord(c) < 128 for c in w))\n",
    "    return asciiish / len(words)\n",
    "\n",
    "def _garble_ratio(lines):\n",
    "    if not lines:\n",
    "        return 0.0\n",
    "    bad = 0\n",
    "    for line in lines:\n",
    "        words = re.findall(r'\\w+', line, flags=re.UNICODE)\n",
    "        if len(words) < 4:\n",
    "            continue\n",
    "        short_ratio = sum(1 for w in words if len(w) <= 2) / max(1, len(words))\n",
    "        digit_ratio = sum(ch.isdigit() for ch in line) / max(1, len(line))\n",
    "        if short_ratio > 0.45 or digit_ratio > 0.35:\n",
    "            bad += 1\n",
    "    return bad / len(lines)\n",
    "\n",
    "def _template_hits(text):\n",
    "    templates = [\n",
    "        'Noi dung cho thay dien bien theo thu tu thoi gian',\n",
    "        'Thong diep duoc rut ra tu cac su kien da xuat hien trong video',\n",
    "        'Khong du du lieu de tao tom tat chi tiet',\n",
    "        'Khong du du lieu de tom tat chi tiet',\n",
    "    ]\n",
    "    lowered = text.lower()\n",
    "    return [t for t in templates if t.lower() in lowered]\n",
    "\n",
    "summary_text = OUTPUT_TEXT.read_text(encoding='utf-8') if OUTPUT_TEXT.exists() else ''\n",
    "internal = _safe_json(INTERNAL_SUMMARY) or {}\n",
    "alignment = _safe_json(ALIGNMENT) or {}\n",
    "quality = _safe_json(REPORT) or {}\n",
    "\n",
    "lines = _split_sentences(summary_text)\n",
    "backend = str((internal.get('generation_meta') or {}).get('backend', 'unknown'))\n",
    "plot = str(internal.get('plot_summary', ''))\n",
    "moral = str(internal.get('moral_lesson', ''))\n",
    "template_matches = _template_hits('\\n'.join([plot, moral, summary_text]))\n",
    "ascii_ratio = _asciiish_ratio(summary_text)\n",
    "garble_ratio = _garble_ratio(lines)\n",
    "\n",
    "blocks = alignment.get('blocks', []) if isinstance(alignment, dict) else []\n",
    "fallback_counter = Counter(str(x.get('fallback_type', '')) for x in blocks if isinstance(x, dict))\n",
    "no_match_rate = 0.0\n",
    "if blocks:\n",
    "    no_match_rate = fallback_counter.get('no_match', 0) / len(blocks)\n",
    "\n",
    "score = 100\n",
    "if backend == 'heuristic':\n",
    "    score -= 35\n",
    "score -= min(25, int(100 * no_match_rate * 0.4))\n",
    "score -= min(20, int(100 * garble_ratio * 0.4))\n",
    "score -= min(20, int(100 * max(0.0, ascii_ratio - 0.55) * 0.5))\n",
    "score -= 10 * len(template_matches)\n",
    "score = max(0, score)\n",
    "\n",
    "severity = 'good' if score >= 80 else ('warning' if score >= 60 else 'poor')\n",
    "\n",
    "print('--- Auto diagnose summary quality ---')\n",
    "print('Run ID:', RUN_ID)\n",
    "print('Backend:', backend)\n",
    "print('Score:', score, f'({severity})')\n",
    "print('No-match rate:', f'{no_match_rate:.3f}')\n",
    "print('Garble ratio:', f'{garble_ratio:.3f}')\n",
    "print('ASCII-ish ratio:', f'{ascii_ratio:.3f}')\n",
    "print('Template hits:', template_matches if template_matches else 'none')\n",
    "if isinstance(quality, dict):\n",
    "    print('QC overall_status:', quality.get('overall_status', 'unknown'))\n",
    "\n",
    "issues = []\n",
    "if backend == 'heuristic':\n",
    "    issues.append('Dang dung heuristic summary; noi dung de bi template hoa.')\n",
    "if no_match_rate > 0.30:\n",
    "    issues.append('Alignment no_match_rate cao, de gay ghiep sai thoai-canh.')\n",
    "if garble_ratio > 0.25:\n",
    "    issues.append('Nhieu cau co dau hieu nhieu ASR noise/vo nghia.')\n",
    "if ascii_ratio > 0.65:\n",
    "    issues.append('Ty le tu ASCII cao, co kha nang tron caption/thoai tieng Anh.')\n",
    "if template_matches:\n",
    "    issues.append('Phat hien cau mau co dinh trong tom tat.')\n",
    "\n",
    "print('Detected issues:')\n",
    "if issues:\n",
    "    for idx, item in enumerate(issues, start=1):\n",
    "        print(f'{idx}. {item}')\n",
    "else:\n",
    "    print('1. Khong phat hien dau hieu bat thuong ro rang theo rule hien tai.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "RUN_DIR = ARTIFACTS_DRIVE / RUN_ID\n",
    "FINAL_DIR = DELIVERABLES_DRIVE / RUN_ID\n",
    "ALIGNMENT = RUN_DIR / 'g2_align' / 'alignment_result.json'\n",
    "SCRIPT = RUN_DIR / 'g5_segment' / 'summary_script.json'\n",
    "MANIFEST = RUN_DIR / 'g5_segment' / 'summary_video_manifest.json'\n",
    "REPORT = RUN_DIR / 'g8_qc' / 'quality_report.json'\n",
    "\n",
    "subprocess.check_call([\n",
    "    sys.executable,\n",
    "    'docs/Reasoning-NLP/schema/validate_artifacts.py',\n",
    "    '--alignment', str(ALIGNMENT),\n",
    "    '--script', str(SCRIPT),\n",
    "    '--manifest', str(MANIFEST),\n",
    "    '--report', str(REPORT),\n",
    "    '--contracts-dir', 'contracts/v1/template',\n",
    "])\n",
    "\n",
    "OUTPUT_VIDEO = FINAL_DIR / 'summary_video.mp4'\n",
    "OUTPUT_TEXT = FINAL_DIR / 'summary_text.txt'\n",
    "print('Output video:', OUTPUT_VIDEO)\n",
    "print('Output text:', OUTPUT_TEXT)\n",
    "print('--- Summary text preview ---')\n",
    "print(OUTPUT_TEXT.read_text(encoding='utf-8'))\n",
    "Video(str(OUTPUT_VIDEO), embed=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "full_pipeline_m1_m2_m3_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
